{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 2.2: Ensemble Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of HW2.2\n",
    "\n",
    "The aim of this assignment is to use Ensemble Learning to solve a problem. In this way, it is aimed to understand the benefits of Ensemble Learning and to teach the usage details of different ensemble approaches with the help of ScikitLearn library. \n",
    "\n",
    "The following methods will be implemented within the scope of this assignment. These are:\n",
    "- Voting Classifier (hard & soft voting) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate moon dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 2.2.1 Voting classifier in Scikit-Learn\n",
    "- Train voting classifiers in Scikit-Learn, composed of at least three diverse classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "clf1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf2 = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "clf3 = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#define your hard voting classifier\n",
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[('rf', clf1), ('et', clf2), ('gb', clf3)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "\n",
    "#train your voting classifier using X_train, y_train\n",
    "voting_clf_hard.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "\n",
    "#write obtained individual classifiers. Compare them with \"Voting Classifiers (hard and soft)\" for \"X_test/y_test\" data\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (clf1, clf2, clf3, voting_clf_hard):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 2.2.2 Bagging/Pasting\n",
    "- One way to get a diverse set of classifiers is to use very different training algorithms\n",
    "- Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set\n",
    "    - When sampling is performed with replacement, this method is called ***bagging*** \n",
    "        - short for ***bootstrap aggregating***\n",
    "    - When sampling is performed without replacement, it is called pasting\n",
    "- Both bagging and pasting allow training instances to be sampled several times across multiple predictors\n",
    "- Only bagging allows training instances to be sampled several times for the same predictor\n",
    "- Predictors can all be trained in parallel, via different CPU cores or even different servers.\n",
    "- Similarly, predictions can be made in parallel.\n",
    "- This is one of the reasons why bagging and pasting are such popular methods: they scale very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClassifier class \n",
    "#  \n",
    "#   (this is an example of bagging, but if you want to use pasting instead, just set bootstrap=False). \n",
    "# The n_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and predictions \n",
    "#   (â€“1 tells Scikit-Learn to use all available cores):\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Write Bagging Classifier and train it using ensemble of 500 Decision Tree classifiers, \n",
    "#  each trained on 100 training instances randomly sampled from the training set with replacement \n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "bag_clf.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "\n",
    "# bagging accuracy score\n",
    "# Using X_test dataset calculate/print Bagging Accuracy Score (using accuracy_score metric)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_bag = bag_clf.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
    "print(f\"Bagging Accuracy Score: {bagging_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Without bagging accuracy score\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculate \"Tree Classifier\" accuracy score\n",
    "# Using X_test dataset calculate/print Tree Classifier Accuracy Score (using accuracy_score metric)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(\"Tree Classifier Accuracy Score:\", accuracy_score(y_test, y_pred_tree))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO 2.2.2 Out-of-Bag evaluation\n",
    "- With bagging, some instances may be sampled several times for any given predictor, \n",
    " while others may not be sampled at all. \n",
    "- By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), \n",
    " where m is the size of the training set\n",
    "- Only about 60% of the training instances are sampled on average for each predictor\n",
    "- The remaining 40% of the training instances that are not sampled are called out-of-bag (oob) instances\n",
    "- Since a predictor never sees the oob instances during training, it can be evaluated on these instances, \n",
    "without the need for a separate validation set or cross-validation\n",
    "- In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier\n",
    " to request an automatic oob evaluation after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#write your out-of-bag (oob) classifier and train it. \n",
    "oob_bag_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the out-of-bag (oob) Bagging Classifier\n",
    "oob_bag_clf.fit(X_train, y_train)\n",
    "\n",
    "# According to this oob evaluation print your oob score for test dataset \n",
    "print(\"Out-of-Bag (oob) Score:\", oob_bag_clf.oob_score_)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate oob bagging accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_oob = oob_bag_clf.predict(X_test)\n",
    "print(\"Out-of-Bag (oob) Bagging Accuracy Score:\", accuracy_score(y_test, y_pred_oob))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TODO 2.2.4 Random Forests\n",
    "- Random Forest is an ensemble of Decision Trees\n",
    "- Generally trained via the bagging method typically with max_samples set to the size of the training set\n",
    "- Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, \n",
    "you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees\n",
    "\n",
    "- Train a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Write your RandomForestClassifier classifier using sklearn RandomForestClassifier class and train it. \n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate/print the prediction accuracy of the rnd_clf for Test Data\n",
    "y_pred_rnd = rnd_clf.predict(X_test)\n",
    "print(\"Random Forest Classifier Accuracy Score:\", accuracy_score(y_test, y_pred_rnd))\n",
    "\n",
    "# Write your RandomForestClassifier classifier using BaggingClassifier equivalent and train it.(estimator=500, leafnode=16) \n",
    "bag_rnd_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_leaf_nodes=16),\n",
    "    n_estimators=500,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the Random Forest Classifier using BaggingClassifier\n",
    "bag_rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate/print the prediction accuracy of the bag_rnd_clf for Test Data\n",
    "y_pred_bag_rnd = bag_rnd_clf.predict(X_test)\n",
    "print(\"Random Forest Classifier (BaggingClassifier) Accuracy Score:\", accuracy_score(y_test, y_pred_bag_rnd))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 2.2.5 Write Your Novel Ensemble Model \n",
    "- Write your customized Ensemble Classifier written by yourself \n",
    "- Try to get the highest score for the same dataset\n",
    "- There no limit. There is no specific constraints. Note that the classifier you write should be an Ensemble Classifier. \n",
    "\n",
    "Note: Students with the highest score on the assignment will be awarded an additional 10 points as an assignment score. All submitted scores will be ranked in descending order and the top 5 students will be awarded an additional +10 points for WH2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.ensemble import BaseEnsemble\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class MyEnsembleClassifier(BaseEnsemble, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, n_estimators=3):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators_ = []\n",
    "        self.weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        # Clear any previous fit results\n",
    "        self.estimators_ = []\n",
    "        self.weights = []\n",
    "\n",
    "        # Fit n_estimators base models\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap resample the data\n",
    "            X_bootstrap, y_bootstrap = resample(X, y, random_state=np.random.randint(1, 1000))\n",
    "\n",
    "            # Clone the base estimator and fit on the resampled data\n",
    "            estimator = clone(self.base_estimator)\n",
    "            estimator.fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "            # Append the trained base estimator to the list of estimators\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "            # Assign equal weight to each base model (you can customize weights based on performance)\n",
    "            self.weights.append(1.0 / self.n_estimators)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check if the classifier has been fitted\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Make predictions from all base models\n",
    "        predictions = np.array([estimator.predict(X) for estimator in self.estimators_])\n",
    "\n",
    "        # Apply the weighted majority voting\n",
    "        final_predictions = np.average(predictions, axis=0, weights=self.weights)\n",
    "\n",
    "        # Convert probabilities to binary predictions\n",
    "        binary_predictions = (final_predictions >= 0.5).astype(int)\n",
    "\n",
    "        return binary_predictions\n",
    "\n",
    "    \n",
    "# Write your Ensemble Classifier and train it. \n",
    "\n",
    "my_base_estimator = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "my_ensemble_clf = MyEnsembleClassifier(base_estimator=my_base_estimator, n_estimators=5)\n",
    "\n",
    "# Train your custom ensemble classifier\n",
    "my_ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_ensemble = my_ensemble_clf.predict(X_test)\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(\"Custom Ensemble Classifier Accuracy Score:\", accuracy_ensemble)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
